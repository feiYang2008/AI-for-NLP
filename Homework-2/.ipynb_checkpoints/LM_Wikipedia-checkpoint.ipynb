{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import jieba\n",
    "import collections\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = '/home/yikang/Desktop/TextData/wikiarticles'\n",
    "all_articles = glob.glob(home + '/*/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    \"\"\"\n",
    "    @string: `string`\n",
    "    @return: `string`, string after cleaning the non-words/non-digits characters\n",
    "    \"\"\"\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))\n",
    "\n",
    "def merge2(gen):\n",
    "    \"\"\"\n",
    "    merget adjacent words in the `generator`\n",
    "    @gen: `generator(string)`, a generator of string containing words\n",
    "    @return: `generator(string)`, a generator of string\n",
    "    \"\"\"\n",
    "    for i in gen:\n",
    "        nex = next(gen)\n",
    "        yield i + nex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_47 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_84 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_27 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_78 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_33 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_62 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_65 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_95 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_40 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_36 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_96 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_85 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_87 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_28 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_59 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_43 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_19 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_52 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_00 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_64 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_14 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_49 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_20 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_60 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_81 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_01 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_74 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_50 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_06 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_30 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_09 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_79 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_88 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_03 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_21 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_37 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_38 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_90 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_15 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_58 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_23 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_32 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_17 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_98 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_31 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_80 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_54 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_75 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_48 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_04 complete\n"
     ]
    }
   ],
   "source": [
    "# load articles(partial)\n",
    "TEXT = \"\"\n",
    "for file in all_articles[:50]:\n",
    "    print(file, \"complete\")\n",
    "    TEXT += token(open(file, 'r').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.622 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# build corpus\n",
    "Text_seg_1 = jieba.cut(TEXT)                    # segment Text to 1-grams\n",
    "vocabulary = collections.Counter(Text_seg_1)    # frequencies for 1-grams\n",
    "Text_seg_2 = merge2(jieba.cut(TEXT))            # segement Text to 2-grams\n",
    "vocabulary_2 = collections.Counter(Text_seg_2)  # frequencies for 2-grams\n",
    "\n",
    "# compute the sum of frequencies\n",
    "_gram_1 = sum(vocabulary.values())\n",
    "_gram_2 = sum(vocabulary_2.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram probablity\n",
    "def get_prob(word):\n",
    "    \"\"\"\n",
    "    fetch the estimated probability of an 1-gram\n",
    "    @word: `string`\n",
    "    @return: `float`\n",
    "    \"\"\"\n",
    "    esp = 1 / _gram_1\n",
    "    if word in vocabulary:\n",
    "        return vocabulary[word] / _gram_1\n",
    "    else:\n",
    "        return esp\n",
    "\n",
    "# 2-gram probability\n",
    "def get_combination_prob(word1, word2):\n",
    "    \"\"\"\n",
    "    return the joint probability of word1 and word2,\n",
    "    @word1, word2: `string`\n",
    "    @return: `float`, estimated joint probability of w1 and w2 (2-gram)\n",
    "    \"\"\"\n",
    "    if word1 + word2 in vocabulary_2:\n",
    "        return vocabulary_2[word1 + word2] / _gram_2\n",
    "    else:\n",
    "        return 1 / _gram_2\n",
    "\n",
    "def get_prob_2gram(w1, w2):\n",
    "    \"\"\"\n",
    "    @w1, w2: `sttring`.\n",
    "    @return: `float`, conditional probability of w1 and w2 (2-gram/1-gram)\n",
    "    \"\"\"\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "# language model\n",
    "def language_model_of_2gram(sentence):\n",
    "    \"\"\"\n",
    "    return the joint probability of a sequence of words\n",
    "    @sentence: `string`, sequence of words\n",
    "    @return: `float`, the estimated joint probability\n",
    "    \"\"\"\n",
    "    sentence_prob = 1\n",
    "    words = list(jieba.cut(sentence))\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            prev = words[i-1]\n",
    "            prob = get_prob_2gram(prev, word)\n",
    "        sentence_prob *= prob\n",
    "    return sentence_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.332680942136675e-10\n",
      "8.917831470044178e-12\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(language_model_of_2gram(\"明天去北京\"))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    \"\"\"\n",
    "    @string: `string`\n",
    "    @return: `string`, string after cleaning the non-words/non-digits characters\n",
    "    \"\"\"\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))\n",
    "\n",
    "def merge2(gen):\n",
    "    \"\"\"\n",
    "    merget adjacent words in the `generator`\n",
    "    @gen: `generator(string)`, a generator of string containing words\n",
    "    @return: `generator(string)`, a generator of string\n",
    "    \"\"\"\n",
    "    for i in gen:\n",
    "        nex = next(gen)\n",
    "        yield i + nex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import jieba\n",
    "import collections\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Language Model for Wikipedia data\n",
    "\n",
    " * Load Wikipedia Dataset\n",
    " * Remove the characters that are non-word or non-digital\n",
    " * Build language model based on 2-gram\n",
    " * Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = '/home/yikang/Desktop/TextData/wikiarticles'\n",
    "all_articles = glob.glob(home + '/*/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    \"\"\"\n",
    "    @string: `string`\n",
    "    @return: `string`, string after cleaning the non-words/non-digits characters\n",
    "    \"\"\"\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))\n",
    "\n",
    "def merge2(gen):\n",
    "    \"\"\"\n",
    "    merget adjacent words in the `generator`\n",
    "    @gen: `generator(string)`, a generator of string containing words\n",
    "    @return: `generator(string)`, a generator of string\n",
    "    \"\"\"\n",
    "    for i in gen:\n",
    "        nex = next(gen)\n",
    "        yield i + nex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_47 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_84 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_27 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_78 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_33 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_62 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_65 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_95 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_40 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_36 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_96 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_85 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_87 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_28 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_59 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_43 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_19 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_52 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_00 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_64 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_14 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_49 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_20 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_60 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_81 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_01 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_74 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_50 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_06 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_30 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_09 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_79 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_88 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_03 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_21 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_37 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_38 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_90 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_15 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_58 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_23 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_32 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_17 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_98 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_31 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_80 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_54 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_75 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_48 complete\n",
      "/home/yikang/Desktop/TextData/wikiarticles/AH/wiki_04 complete\n"
     ]
    }
   ],
   "source": [
    "# load articles(partial)\n",
    "TEXT = \"\"\n",
    "for file in all_articles[:50]:\n",
    "    print(file, \"complete\")\n",
    "    TEXT += token(open(file, 'r').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Clean and build word Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus\n",
    "Text_seg_1 = jieba.cut(TEXT)                    # segment Text to 1-grams\n",
    "vocabulary = collections.Counter(Text_seg_1)    # frequencies for 1-grams\n",
    "Text_seg_2 = merge2(jieba.cut(TEXT))            # segement Text to 2-grams\n",
    "vocabulary_2 = collections.Counter(Text_seg_2)  # frequencies for 2-grams\n",
    "\n",
    "# compute the sum of frequencies\n",
    "_gram_1 = sum(vocabulary.values())\n",
    "_gram_2 = sum(vocabulary_2.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Build Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram probablity\n",
    "def get_prob(word):\n",
    "    \"\"\"\n",
    "    fetch the estimated probability of an 1-gram\n",
    "    @word: `string`\n",
    "    @return: `float`\n",
    "    \"\"\"\n",
    "    esp = 1 / _gram_1\n",
    "    if word in vocabulary:\n",
    "        return vocabulary[word] / _gram_1\n",
    "    else:\n",
    "        return esp\n",
    "\n",
    "# 2-gram probability\n",
    "def get_combination_prob(word1, word2):\n",
    "    \"\"\"\n",
    "    return the joint probability of word1 and word2,\n",
    "    @word1, word2: `string`\n",
    "    @return: `float`, estimated joint probability of w1 and w2 (2-gram)\n",
    "    \"\"\"\n",
    "    if word1 + word2 in vocabulary_2:\n",
    "        return vocabulary_2[word1 + word2] / _gram_2\n",
    "    else:\n",
    "        return 1 / _gram_2\n",
    "\n",
    "def get_prob_2gram(w1, w2):\n",
    "    \"\"\"\n",
    "    @w1, w2: `sttring`.\n",
    "    @return: `float`, conditional probability of w1 and w2 (2-gram/1-gram)\n",
    "    \"\"\"\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "# language model\n",
    "def language_model_of_2gram(sentence):\n",
    "    \"\"\"\n",
    "    return the joint probability of a sequence of words\n",
    "    @sentence: `string`, sequence of words\n",
    "    @return: `float`, the estimated joint probability\n",
    "    \"\"\"\n",
    "    sentence_prob = 1\n",
    "    words = list(jieba.cut(sentence))\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            prev = words[i-1]\n",
    "            prob = get_prob_2gram(prev, word)\n",
    "        sentence_prob *= prob\n",
    "    return sentence_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.332680942136675e-10\n",
      "8.917831470044178e-12\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(language_model_of_2gram(\"明天去北京\"))\n",
    "print(language_model_of_2gram(\"看没看新闻\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Applications \n",
    "# * Voice Recognization\n",
    "# * Sogou pinyin input\n",
    "#   * 通过输入的首字母序列，从左到右用rolling window的形式来从数据库中搜寻最大可能的词语段。\n",
    "# * Auto correction in search engine\n",
    "#   * 将输入的句子分割成片段，计算各个片段的再词库中的概率，对概率最低的片段进行修改。\n",
    "# * Abnormal Detection\n",
    "#   * 针对不同用户的行为数据/用词顺序建模，记录习惯，当该用户多次进行违反行为习惯的操作时，即行为顺序的估计概率很低。说明存在非正常操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
